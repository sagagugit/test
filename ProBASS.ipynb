{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKx3VPzmBHSf"
      },
      "source": [
        "# <center>**ProBASS: a language model with sequence and structural features for predicting the effect of mutations on binding affinity**</center>\n",
        "---\n",
        "Here we introduce a model (ProBASS) which is fine-tuned, incorporating features derived from both Protein Language models ESM-2 and ESM-IF1.This model is designed for the prediction of ddGbind values, which serve as indicators of both the sequence and structural attributes of the mutated protein complexes.\n",
        "\n",
        "The model is an efficient way to predict the effect of mutations on protein binding affinity.\n",
        "\n",
        "---\n",
        "\n",
        "**Instructions for users on how to provide the PDB ID of the protein complex and the CSV file which contains the mutation information to Probass**\n",
        "\n",
        "Please input the \"PDB ID\" of the Protein complex under the subcategory Input Data which is required to calculate the binding affinity of the mutations.\n",
        "\n",
        "The user can specify the desired mutations for binding affinity calculation by providing the information in a CSV file named 'Input.csv'. This file should include five columns with the following headers: 'Mutated_chain', 'Partner_chain', 'Wild_type', 'Position', and 'Mutation'.\n",
        "\n",
        "The 'Mutated_chain' and 'Partner_chain' define the interface of the protein complex. 'Wild_type' refers to the original amino acid in the protein complex, 'Position' indicates the location of the desired mutation, and 'Mutation' specifies the amino acid the user wishes to substitute for the wild type.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3k0uM_5fN9K"
      },
      "source": [
        "# Environment Set up for **ProBASS:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ngtlA-3ygDxe"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu113\n",
        "!pip install torch-scatter torch-sparse torch-cluster torch-spline-conv torch-geometric -f https://data.pyg.org/whl/torch-2.0.0+cu113.html\n",
        "!pip install biotite==0.33.0\n",
        "!pip install catboost\n",
        "!pip install git+https://github.com/facebookresearch/esm.git\n",
        "!pip install requests\n",
        "!pip install biopython"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "I_kHuX4TspGw"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import catboost as cb\n",
        "import torch\n",
        "import esm\n",
        "import scipy\n",
        "from numpy import asarray\n",
        "from numpy import savez_compressed\n",
        "import requests\n",
        "from Bio.PDB import PDBParser"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "\n",
        "cd /content/\n",
        "\n",
        "if [ ! -f ProBASS ]; then\n",
        "\n",
        "\n",
        "    # delete the Cold-scanner/ directory if it already exists\n",
        "    if [ -d \"ProBASS/\" ]; then\n",
        "        rm -rf ProBASS/\n",
        "    fi\n",
        "\n",
        "    # download model\n",
        "    git clone https://github.com/sagagugit/ProBASS --quiet\n",
        "    touch ProBASS\n",
        "fi"
      ],
      "metadata": {
        "id": "EsfKhaV9plmm"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Input Data"
      ],
      "metadata": {
        "id": "slMOUZcHMjcR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "_CyrX8RlilBx",
        "outputId": "5f64d120-a98b-4118-feaf-c1e33ea6b16d",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-cf96e2a3-af55-4ad3-bbf3-64d45220a9a9\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-cf96e2a3-af55-4ad3-bbf3-64d45220a9a9\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Input.csv to Input.csv\n"
          ]
        }
      ],
      "source": [
        "#@title Please provide the PDB ID and mutation information\n",
        "PDB = '3OTJ'#@param {type:\"string\"}\n",
        "try:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')\n",
        "  from google.colab import files\n",
        "  uploaded = files.upload()\n",
        "except FileNotFoundError:\n",
        "  print(\"ERROR: \\n Uploading was not successful. Please restart and try to upload the complex again\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Selecting Path"
      ],
      "metadata": {
        "id": "3b4dxngwJ2QA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "%cd ProBASS\n",
        "!cp /content/Input.csv /content/ProBASS"
      ],
      "metadata": {
        "id": "Q647f5cQiqYQ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6IwIHN76la03"
      },
      "source": [
        "# Extracting embeddings from ESM2 and ESM-IF1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KRvtBTBxnYUk"
      },
      "source": [
        "# Extracting Fasta files for wild type, partner chain and mutated PPI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "o7uWQFbrnrSX"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# Define the PDB code\n",
        "PDB_code = PDB  # You can change this to any PDB code\n",
        "\n",
        "# Construct the URL to download the PDB file\n",
        "url = f'http://www.rcsb.org/pdb/download/downloadFile.do?fileFormat=pdb&structureId={PDB_code}'\n",
        "\n",
        "# Send a GET request to fetch the PDB file\n",
        "response = requests.get(url)\n",
        "\n",
        "# Check if the request was successful\n",
        "if response.status_code == 200:\n",
        "    # Save the file to the local filesystem\n",
        "    with open(f'{PDB_code}.pdb', 'wb') as file:\n",
        "        file.write(response.content)\n",
        "    print(f'{PDB_code}.pdb has been downloaded successfully.')\n",
        "else:\n",
        "    print(f'Failed to download {PDB_code}.pdb. Status code: {response.status_code}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define residue name to single-letter code mapping\n",
        "RESIDUE_NAME_TO_LETTER = {\n",
        "    'ALA': 'A', 'ARG': 'R', 'ASN': 'N', 'ASP': 'D', 'CYS': 'C',\n",
        "    'GLU': 'E', 'GLN': 'Q', 'GLY': 'G', 'HIS': 'H', 'ILE': 'I',\n",
        "    'LEU': 'L', 'LYS': 'K', 'MET': 'M', 'PHE': 'F', 'PRO': 'P',\n",
        "    'SER': 'S', 'THR': 'T', 'TRP': 'W', 'TYR': 'Y', 'VAL': 'V'\n",
        "}\n",
        "\n",
        "# Load the CSV file\n",
        "input_csv = 'Input.csv'\n",
        "df = pd.read_csv(input_csv)\n",
        "\n",
        "# Define PDB code\n",
        "PDB_code = PDB\n",
        "\n",
        "# Read the PDB file\n",
        "pdb_file = f'{PDB_code}.pdb'\n",
        "parser = PDBParser(QUIET=True)\n",
        "structure = parser.get_structure(PDB_code, pdb_file)\n",
        "\n",
        "# Function to extract sequence for a given chain\n",
        "def extract_sequence(chain_id):\n",
        "    sequence = []\n",
        "    for model in structure:\n",
        "        for chain in model:\n",
        "            if chain.get_id() == chain_id:\n",
        "                for residue in chain:\n",
        "                    resname = residue.get_resname()\n",
        "                    if resname in RESIDUE_NAME_TO_LETTER:\n",
        "                        sequence.append(RESIDUE_NAME_TO_LETTER[resname])\n",
        "    return ''.join(sequence)\n",
        "\n",
        "# Create FASTA files for each chain\n",
        "for index, row in df.iterrows():\n",
        "    mutated_chain_id = row['Mutated_chain']\n",
        "    partner_chain_id = row['Partner_chain']\n",
        "\n",
        "    mutated_sequence = extract_sequence(mutated_chain_id)\n",
        "    partner_sequence = extract_sequence(partner_chain_id)\n",
        "\n",
        "    # Save the sequences to FASTA files\n",
        "    with open(f'{PDB_code}_wild.fasta', 'w') as f:\n",
        "        f.write(f'> {PDB_code}_wild\\n{mutated_sequence}\\n')\n",
        "\n",
        "    with open(f'{PDB_code}_partner.fasta', 'w') as f:\n",
        "        f.write(f'> {PDB_code}_partner\\n{partner_sequence}\\n')\n",
        "\n",
        "print(f'FASTA files for {PDB_code} have been created.')"
      ],
      "metadata": {
        "id": "TjpHiKgIjjPk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29a706e3-28c9-4b9d-a3c6-606f453c60e8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FASTA files for 3OTJ have been created.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the PDB code\n",
        "PDB_code = PDB\n",
        "\n",
        "# Load the Input.csv file\n",
        "input_csv = 'Input.csv'\n",
        "df = pd.read_csv(input_csv)\n",
        "\n",
        "# Load the wild-type sequence from the FASTA file\n",
        "def read_fasta(fasta_file):\n",
        "    with open(fasta_file, 'r') as f:\n",
        "        lines = f.readlines()\n",
        "    sequence = ''.join(line.strip() for line in lines[1:])\n",
        "    return sequence\n",
        "\n",
        "wild_sequence = read_fasta(f'{PDB_code}_wild.fasta')\n",
        "\n",
        "# Convert 1-based position to 0-based index for Python\n",
        "def position_to_index(position):\n",
        "    return position - 1\n",
        "\n",
        "# Create a list to store FASTA entries\n",
        "fasta_entries = []\n",
        "\n",
        "for index, row in df.iterrows():\n",
        "    mutated_sequence = list(wild_sequence)\n",
        "    position = row['Position']\n",
        "    mutation = row['Mutation']\n",
        "\n",
        "    # Convert position to 0-based index\n",
        "    idx = position_to_index(position)\n",
        "\n",
        "    # Apply mutation\n",
        "    if 0 <= idx < len(mutated_sequence):\n",
        "        mutated_sequence[idx] = mutation\n",
        "\n",
        "    # Convert list back to string\n",
        "    mutated_sequence_str = ''.join(mutated_sequence)\n",
        "\n",
        "    # Format the FASTA header\n",
        "    fasta_header = f'> {PDB_code}_{position}{mutation}\\n'\n",
        "\n",
        "    # Add to FASTA entries\n",
        "    fasta_entries.append(fasta_header + mutated_sequence_str + '\\n')\n",
        "\n",
        "with open(f'{PDB_code}.fasta', 'w') as f:\n",
        "    f.writelines(fasta_entries)\n",
        "\n",
        "print(f'Mutated FASTA file for {PDB_code} has been created with formatted headers.')\n",
        "\n"
      ],
      "metadata": {
        "id": "tMZMuaJXjonO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d63489e7-233e-4325-94b1-ced5d51ea103"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mutated FASTA file for 3OTJ has been created with formatted headers.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOspFrP_qUhl"
      },
      "source": [
        "# Extract sequence embeddings and Structural embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "AEMbn79PqkWj"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "#Extrat seqeunce embeddings from ESM2 seperately for wild type, partner and mutated PPI\n",
        "!python extract.py esm2_t33_650M_UR50D 3OTJ.fasta 3OTJ_esm2 --repr_layers 0 32 33 --include mean per_tok\n",
        "\n",
        "!python extract.py esm2_t33_650M_UR50D 3OTJ_wild.fasta 3OTJ_esm2_wild --repr_layers 0 32 33 --include mean per_tok\n",
        "\n",
        "!python extract.py esm2_t33_650M_UR50D 3OTJ_partner.fasta 3OTJ_esm2_partner --repr_layers 0 32 33 --include mean per_tok\n",
        "\n",
        "model, alphabet = esm.pretrained.esm_if1_gvp4_t16_142M_UR50()\n",
        "model = model.eval()\n",
        "\n",
        "fpath = PDB + '.pdb'\n",
        "input_file = 'Input.csv'\n",
        "df = pd.read_csv(input_file)\n",
        "\n",
        "chain_ids = list(set(df['Mutated_chain'].tolist() + df['Partner_chain'].tolist()))\n",
        "structure = esm.inverse_folding.util.load_structure(fpath, chain_ids)\n",
        "coords, native_seqs = esm.inverse_folding.multichain_util.extract_coords_from_complex(structure)\n",
        "\n",
        "print(f'Loaded chains: {list(coords.keys())}\\n')\n",
        "\n",
        "for chain_id in chain_ids:\n",
        "    print(f'Chain {chain_id} native sequence:')\n",
        "    print(native_seqs[chain_id])\n",
        "    print('\\n')\n",
        "\n",
        "\n",
        "mutated_chain_ids = df['Mutated_chain'].unique()\n",
        "\n",
        "\n",
        "target_chain_id = mutated_chain_ids[0]\n",
        "rep = esm.inverse_folding.multichain_util.get_encoder_output_for_complex(model, alphabet, coords, target_chain_id)\n",
        "len(coords), rep.shape\n",
        "print(len(coords), rep.shape)\n",
        "print(target_chain_id)\n",
        "\n",
        "numpy_rep =rep.detach().numpy()\n",
        "print(numpy_rep)\n",
        "np.savez(f\"inverse_{PDB}.npz\", data=numpy_rep)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OagSSYQMrRLK"
      },
      "source": [
        "# Run ProBASS to predict the ΔΔG values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "2qa-uEhrrw1V"
      },
      "outputs": [],
      "source": [
        "PDBS = PDB_code = [PDB]\n",
        "\n",
        "def exctracting_embeddings_esm2(pdb):\n",
        "    mutations2= []\n",
        "    Xs2 = []\n",
        "    for header2, _seq2 in esm.data.read_fasta(FASTA_PATH2):\n",
        "        scaled_effect2 = header2.split('|')[-1]\n",
        "        mutations2.append(scaled_effect2)\n",
        "        fn = f'{EMB_PATH2}/{header2[1:]}.pt'\n",
        "        embs2 = torch.load(fn)\n",
        "        Xs2.append(embs2['representations'][33])\n",
        "    Xs2 = torch.stack(Xs2, dim=0).numpy()\n",
        "\n",
        "    return Xs2, mutations2\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def exctracting_embeddings_esm2_wild(pdb):\n",
        "    mutations2_w= []\n",
        "    Xs2_w = []\n",
        "    for header2, _seq2 in esm.data.read_fasta(FASTA_PATH2_w):\n",
        "        scaled_effect2_w = header2.split('|')[-1]\n",
        "        mutations2_w.append(scaled_effect2_w)\n",
        "        fn = f'{EMB_PATH2_w}/{header2[1:]}.pt'\n",
        "        embs2 = torch.load(fn)\n",
        "        Xs2_w.append(embs2['representations'][33])\n",
        "    Xs2_w = torch.stack(Xs2_w, dim=0).numpy()\n",
        "\n",
        "    return Xs2_w\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def exctracting_embeddings_esm2_bind(pdb):\n",
        "    mutations2_b= []\n",
        "    Xs2_b = []\n",
        "    for header2, _seq2 in esm.data.read_fasta(FASTA_PATH2_b):\n",
        "        scaled_effect2_b = header2.split('|')[-1]\n",
        "        mutations2_b.append(scaled_effect2_b)\n",
        "        fn = f'{EMB_PATH2_b}/{header2[1:]}.pt'\n",
        "        embs2 = torch.load(fn)\n",
        "        Xs2_b.append(embs2['representations'][33])\n",
        "    Xs2_b = torch.stack(Xs2_b, dim=0).numpy()\n",
        "\n",
        "    return Xs2_b\n",
        "\n",
        "def exctracting_embeddings_1f(pdb):\n",
        "    temp= np.load(inverse_path)\n",
        "    inverse= temp['data']\n",
        "\n",
        "\n",
        "    average_mean_embedding = np.mean(inverse, axis=0)\n",
        "    average_mean_embedding.shape\n",
        "    inverse_mean_reshape = average_mean_embedding.reshape([1, 512])\n",
        "    inverse_mean_reshape.shape\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    return inverse_mean_reshape"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "ddg_values = []\n",
        "embeddings = []\n",
        "for pdb in PDBS:\n",
        "    FASTA_PATH = \"/content/ProBASS/{}.fasta\".format(pdb)\n",
        "    EMB_PATH = \"/content/ProBASS/{}_1V\".format(pdb)\n",
        "    FASTA_PATH2 = \"/content/ProBASS/{}.fasta\".format(pdb)\n",
        "    EMB_PATH2 = \"/content/ProBASS/{}_esm2\".format(pdb)\n",
        "    FASTA_PATH_w = \"/content/ProBASS/{}_wild.fasta\".format(pdb)\n",
        "    EMB_PATH_w = \"/content/ProBASS/{}_1V_wild\".format(pdb)\n",
        "    FASTA_PATH2_w = \"/content/ProBASS/{}_wild.fasta\".format(pdb)\n",
        "    EMB_PATH2_w = \"/content/ProBASS/{}_esm2_wild\".format(pdb)\n",
        "    FASTA_PATH_b = \"/content/ProBASS/{}_partner.fasta\".format(pdb)\n",
        "    EMB_PATH_b = \"/content/ProBASS/{}_1V_partner\".format(pdb)\n",
        "    FASTA_PATH2_b = \"/content/ProBASS/{}_partner.fasta\".format(pdb)\n",
        "    EMB_PATH2_b = \"/content/ProBASS/{}_esm2_partner\".format(pdb)\n",
        "    inverse_path = '/content/ProBASS/inverse_{}.npz'.format(pdb)\n",
        "    csv_path = \"/content/ProBASS/{}.csv\".format(pdb)\n",
        "    Xs2, mutations2= exctracting_embeddings_esm2(pdb)\n",
        "    Xs2_w= exctracting_embeddings_esm2_wild(pdb)\n",
        "    Xs2_w=np.tile(Xs2_w, (len(Xs2), 1, 1))\n",
        "    Xs2_b=exctracting_embeddings_esm2_bind(pdb)\n",
        "    Xs2_b=np.tile(Xs2_b, (len(Xs2), 1, 1))\n",
        "    inverse=exctracting_embeddings_1f(pdb)\n",
        "    inverse=np.tile(inverse, (len(Xs2), 1))\n",
        "    mutant_and_partner_together_esm2 = np.concatenate([Xs2_b, Xs2], axis =1)\n",
        "\n",
        "    wild_type_and_partner_together_esm2 = np.concatenate([Xs2_b, Xs2_w], axis =1)\n",
        "    mutant_and_partner_together_esm2_mean=np.mean(mutant_and_partner_together_esm2, axis=1)\n",
        "    wild_type_and_partner_together_esm2_mean=np.mean(wild_type_and_partner_together_esm2, axis=1)\n",
        "    ddg_1v = np.subtract(mutant_and_partner_together_esm2_mean, wild_type_and_partner_together_esm2_mean)\n",
        "\n",
        "    ddg_esm2_with_inverse = np.concatenate([ddg_1v, inverse], axis =1)\n",
        "    embeddings.append(ddg_esm2_with_inverse)"
      ],
      "metadata": {
        "id": "8vouJu42j-Un"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "ddg_length = len(embeddings[0])\n",
        "ddg_values = [0] * ddg_length\n",
        "\n",
        "\n",
        "flattened_list = ddg_values\n",
        "\n",
        "\n",
        "extracted_array = embeddings[0]\n",
        "Xs_test = extracted_array\n",
        "ys_test = flattened_list\n",
        "\n",
        "np.savez('test.npz', data=Xs_test, label=ys_test)"
      ],
      "metadata": {
        "id": "fb0ixxAVkBm9"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "temp = np.load('test.npz')\n",
        "X_test, test_y = temp['data'], temp['label']"
      ],
      "metadata": {
        "id": "w10iJgAKkDkw"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Load your Input.csv\n",
        "input_data = pd.read_csv('Input.csv')\n",
        "#Load your model and make predictions\n",
        "model = cb.CatBoostRegressor()\n",
        "loaded_model1 = cb.CatBoostRegressor()\n",
        "loaded_model1.load_model('Probass_model.cbm')\n",
        "\n",
        "ypred = loaded_model1.predict(X_test)\n",
        "\n",
        "\n",
        "input_data['Mutation'] = input_data['Wild_type'] + input_data['Position'].astype(str) + input_data['Mutation']\n",
        "\n",
        "predicted_df = pd.DataFrame({'Mutation': input_data['Mutation'], 'predicted_value': ypred})\n",
        "\n",
        "predicted_df.to_csv('predicted_values.csv', index=False)\n"
      ],
      "metadata": {
        "id": "-tCwzqKfkFXM"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download Predicted Binding Affinintes"
      ],
      "metadata": {
        "id": "R9npsnXKnHou"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import shutil\n",
        "files.download('predicted_values.csv')"
      ],
      "metadata": {
        "id": "gVtZdV_XnMRI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "971cbfb3-57cd-44f4-8dc5-9bb11a3e0bcc"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_e903912e-a1b6-479c-bf50-bb79516d61ed\", \"predicted_values.csv\", 72)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cxj6-6kwNvhV"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "I3k0uM_5fN9K",
        "3b4dxngwJ2QA",
        "KRvtBTBxnYUk",
        "oOspFrP_qUhl",
        "OagSSYQMrRLK",
        "R9npsnXKnHou"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}